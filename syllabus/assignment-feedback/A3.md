# Assignment 3

In general a very good job albeit a few groups didn't hand in.

The primary feedback is the evaluation metric. Most people used the sklearn (which I na√Øvely recommended). In fact this was wrong of me it turns out you should have used seqeval instead. 

What is the difference between these two? Well seqeval ignores the "O" and combines the IOB-tags such that "I-PER" and "B-PER" is in fact the same category.

```
from sklearn.metrics import classification_report

y_true = ['O', 'O', 'B-MISC', 'I-MISC', 'B-MISC', 'O', 'O', 'B-PER', 'I-PER', 'O']
y_pred = ['O', 'O', 'B-MISC', 'I-MISC', 'B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O']

print(classification_report(y_true, y_pred))

              precision    recall  f1-score   support

      B-MISC       1.00      1.00      1.00         2
       B-PER       1.00      1.00      1.00         1
      I-MISC       0.50      1.00      0.67         1
       I-PER       1.00      1.00      1.00         1
           O       1.00      0.80      0.89         5

    accuracy                           0.90        10
   macro avg       0.90      0.96      0.91        10
weighted avg       0.95      0.90      *0.91*        10
```

```
from seqeval.metrics import classification_report

y_true = [['O', 'O', 'B-MISC', 'I-MISC', 'B-MISC', 'O', 'O', 'B-PER', 'I-PER', 'O']]
y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O']]

print(classification_report(y_true, y_pred))

              precision    recall  f1-score   support

        MISC       0.50      0.50      0.50         2
         PER       1.00      1.00      1.00         1

   micro avg       0.67      0.67      0.67         3
   macro avg       0.75      0.75      0.75         3
weighted avg       0.67      0.67      *0.67*         3
```