

# Estimating the idf on the test set
Many of you estimated the idf on the test set. This does not seem to influence performance too severely (surprisingly), however it does mean that your model is trained on data scaled differntly compared to the train set. For instance imagine our test set was one document:

```
Logarithms is useful in many areas of science
```

Here the idf would be be constant for all the words i.e. $log(\frac{1}{1+1}) = -0.3$ as each word appear once in the document and the document is the whole corpus. This would naturally not correspond with the scaling of the words used for our train set. For instance we could imagine that the word `is` appear in approximately half of our training corpus and assuming this contain 200 documents we have $log(\frac{200}{100+1}) = 0.3$. Thus 
Thus there reason why we don't see a noticeably drop in performance might be due to 

# List of modules and nn.Sequential
Mulitple of you create layers like linear1, linear2, linear3 and while this work for relatively small networks it might be nice to be introduced to [nn.ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html), which allow you to create list of modules. This for example allows you to create arbitrary length neural networks:

```py
class Net(nn.Module):
    def __init__(self, layers=[784,30, 30, 10]):
        super().__init__()

        self.layers = nn.ModuleList()
        for input_size, ouput_size in zip(layers[:-1], layers[1:]):
            layer = nn.Linear(input_size, output_size)
            self.layers.append(layer)


    def forward(self, x):
        for layer is self.layers:
            x = layer(x)
            x = torch.relu(x)
        return x
```

Alternatively I have also seen a few use [nn.sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html?highlight=sequential#torch.nn.Sequential) as a list. This is generally not the use case of a sequential model.

```py
# define model
model = nn.Sequential(
          nn.Linear(784, 30),
          nn.ReLU(),
          nn.Linear(30,10),
          nn.LogSoftmax()
        )

# forward pass (through the entire model)
x = model(x)
```

The advantage of this model it that it is quick to make, but the disadvantage is that it is not very flexible. For instance we can't add a `.fit` method to the model.

# What is up with log10?
Seemingly a lot of people have used log10 in your tf-idf. I know it is on Ross' slide but it doesn't really make much of a difference. Why is it that it does not make a change you might ask?

Well that is because logarithms in any base is proportional to eachother. i.e.

$log_e(x) = C \cdot log_{10}(x)$

Where $C$ is a constant.

The reason for this is best shown using an example. As you know $log_e(x)$ (or simetimes called $ln$) is the inverse of $e^x$ thus $log_e(e^x) = x$, however logs have the convenient ability (regardless its base) that $log(x^y) = y \cdot log(x)$ and thus if we where to use $log_10$ on $e^x$ we would get $log_10(e^x) = x \cdot log_10(e)$. Notice that here $log_10(e)$ is just a constant.

But then are you saying that scaling doesn't matter? Well no of course it does, but essentially you are just scaling the wieghts of the model (take a minute to ponder why that is. *Hint* try imagine a simple 1 layer neural network). That being said as you know many models train better with scaled variables and that includes neural networks, and there might be an argument for base 10 resulting in a better scaling.


# Results of experiments
Many of you have had some interesting conclusions from your experiment and thought it would be a shame not to share it. Do not that these conclusions naturally depend on implementations details.

- Logistic regression typically performed worse than the neural network, but typically no more than two percentage points.
- Neural networks with more and larger layers performed better across implementations.
- Lower learning rates typically lead to better performance with the lowest tested so far being `0.001` using the `AdamW` optimizer.
- Using only nouns (and prober nouns) decreased performance, but not by a large margin. Performing surprisingly competitively.
- Batches makes the whole thing run much faster!
- Log normalized frequencies that is ($log(n_w + 1)$), where $n_w$ is the number of occurances of a given word, $w$ seems to perform better than term frequencies. Although I could imagine that this also depends on other implementation details and other factors like the length of sequences.




