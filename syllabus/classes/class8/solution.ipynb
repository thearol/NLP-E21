{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises on Attention\n",
    "In the following exercises, we will take a look at dot product attention, we will use word embeddings for this, however these would in the context of encoder-decoder models be hidden states from the decoder and encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Calculate the dot product between two word embeddings that you believe are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.078485"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.dot(model[\"goose\"], model[\"duck\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Calculate the dot product between the two word and a word which you believe is dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.842581"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(model[\"goose\"], model[\"moose\"])\n",
    "np.dot(model[\"duck\"], model[\"moose\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) make the three words into a matrix $E$ and multiply it with its own transpose using matrix multiplication. So $E \\cdot E^T$\n",
    "  - what do the entries in the matrix correspond to? What do you imagine the dot product is? *Hint*, the similarity between vectors (cosine similarity) is exactly the same as the dot product assuming you normalize the length of each vector to 1.\n",
    "\n",
    "Following the article by Lindsay et al (2020) this is one way (out of many) to implement the attention mechanism denoted ${\\tilde{\\alpha_t}} = \\phi(h_{t-1}, c) = h_{t-1} \\cdot c^T$ (3.1) called dot product attention. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.631552, 16.078485, 14.530851],\n",
       "       [16.078485, 23.134829, 11.84258 ],\n",
       "       [14.530851, 11.84258 , 16.661507]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = model[\"goose\", \"duck\", \"moose\"]\n",
    "\n",
    "E @ E.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Assume $h_{t-1}$ our previous hidden state is the word vector for \"hei\", calculate the attention the $h_{t-1}$ and the embeddings $\\mathcal{E}_{greetings}$, $\\mathcal{E}_{hello}$, $\\mathcal{E}_{jungle}$. Remember that the attention weight is given by $\\boldsymbol{\\alpha_t} = softmax(\\boldsymbol{\\tilde{\\alpha_t}})$ (3.2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-9103eb445928>:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention = softmax(attn_)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "import torch\n",
    "\n",
    "\n",
    "ht = model[\"hei\"]\n",
    "\n",
    "E = model[\"greetings\", \"hello\", \"jungle\"]\n",
    "\n",
    "attn_ = ht @ E.T\n",
    "attn_ = torch.tensor(attn_)\n",
    "\n",
    "attention = softmax(attn_)\n",
    "attention = attention.view(1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) The matrix resulting from the softmax is referred to as the attention matrix and is how much each embedding should be weighted (paid attention to) when we multiply our attention matrix by our matrix embeddings consisting of $\\mathcal{E}_{greetings}$, $\\mathcal{E}_{hello}$, $\\mathcal{E}_{jungle}$. Try it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_from_5 = attention @ E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5.1) This is essentially a weighted sum, one way to see this is to extract the weight from the first row of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1684, 0.7876, 0.0441])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = attention[0] # first row of our attention matrix\n",
    "print(attn)\n",
    "weighted = attn[0] * model[\"greetings\"] + attn[1] * model[\"hello\"] + attn[2] * model[\"jungle\"]\n",
    "\n",
    "# compare the weighted embedding with the first column in the matrix from 5).\n",
    "\n",
    "matrix_from_5[0] - weighted < 000.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 6.1) Examine the attention matrix, which words have a higher attention weight and why?\n",
    "\n",
    "*Hint* you can plot the matrix using:\n",
    "```\n",
    "H = model[\"man\", \"banana\"]\n",
    "C = model[[\"woman\", \"apple\", \"pineapple\"]\n",
    "wieghted, attn = attention(H, C)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(attn)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, value, scale=False):\n",
    "    \"Compute (Scaled) Dot Product Attention'\"\n",
    "    query = torch.tensor(query)\n",
    "    value = torch.tensor(value)\n",
    "    d_k = query.shape[-1]\n",
    "    if scale:\n",
    "        scores = torch.matmul(query, value.T) / np.sqrt(d_k)\n",
    "    else:\n",
    "        scores = torch.matmul(query, value.T) \n",
    "    p_attn = softmax(scores)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-60fba23f5a71>:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p_attn = softmax(scores)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = model[\"man\", \"banana\"]\n",
    "C = model[\"woman\", \"apple\", \"pineapple\"]\n",
    "scores, attn = attention(H, C)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(attn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-60fba23f5a71>:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p_attn = softmax(scores)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "scores, attn = attention(H, C, scale = True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(attn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "021482b7625aaacc2d343324781c6ce2f121934a239bde69eda2b56fdffea080"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('NLP': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
