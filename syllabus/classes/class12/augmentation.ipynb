{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation using Augmentation\n",
    "For this class we will conduct model validation using augmentation, we will especially use the package [Augmenty](https://kennethenevoldsen.github.io/augmenty/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We will need to set up a few things before we start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages:\n",
    "For this tutorial you will need the following packages:\n",
    "\n",
    "- spaCy and augmenty are used for the augmentation\n",
    "- transformers are use to run the model we wish to validate\n",
    "- danlp is used to download the dataset we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-5a5edb48c1f0>, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5a5edb48c1f0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    spacy -m download da_core_news_lg\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# !pip install augmenty spacy==3.1.1 transformers==4.2.2 danlp==0.0.12\n",
    "# !python -m spacy download da_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "For this dataset we will be using [DKHate](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dkhate). The DKHate dataset contains user-generated comments from social media platforms (Facebook and Reddit) annotated for various types and target of offensive language. Note that only labels for the sub-task A (Offensive language identification), i.e. NOT (Not Offensive) / OFF (Offensive), are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import DKHate\n",
    "import pandas as pd\n",
    "dkhate = DKHate()\n",
    "test, train = dkhate.load_with_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make everything run faster we will only be using a subsample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 20\n",
    "\n",
    "# make sure to sample evenly from the two samples\n",
    "n_labels = len(test[\"subtask_a\"].unique())\n",
    "samples_pr_lab = samples//n_labels\n",
    "\n",
    "off = test[test[\"subtask_a\"] == \"OFF\"].sample(samples_pr_lab)\n",
    "not_off = test[test[\"subtask_a\"] == \"NOT\"].sample(samples_pr_lab)\n",
    "mini_test = pd.concat([off, not_off])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the data using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                  tweet subtask_a\nid                                                               \n3299  Det er kraftedme en stor præstation der her, e...       OFF\n305   De små får fri på vores skole fordi en knægt b...       OFF\n2041  HEY! Bilar er jo sådan det eneste gode der er ...       OFF\n701   @USER hvis hun ikke kan koge pastaen rigtigt, ...       OFF\n962   Passiv aggressiv måde at kalde dig for et pikfjæs       OFF\n799   hvorfor i den fucking store helvede skal man f...       OFF\n519   Det sgu heller ikke okay. jeg havde sgu også b...       OFF\n987   Tak, fordi du ikke vanærede @USER ved at sætte...       OFF\n1251  Han EJER ikke respekt for nogen eller noget......       OFF\n1167         Lækkert lorteindslag v1, jeg giver d1 1/1.       OFF\n581   Biograferne tjener absolut intet på billetten ...       NOT\n2643                                                lol       NOT\n1889                    ###DANSKEN ER EN DEJLIG MAND!!!       NOT\n632   DET ER helt fint at de mænd der gerne vil have...       NOT\n380   @USER siger det går ufatteligt godt i Danmark....       NOT\n1431  @USER, du snakker udenom, og du gør det med vi...       NOT\n2080  Hvis jeg elsker Danmark og hader gud er det så...       NOT\n633   Og hvad med Danelagen? St. Croix, St. Thomas o...       NOT\n23    1 million sort om året, sætter sit præg på til...       NOT\n1038   Endnu en af Stampes berigere… Få dem dog ud!....       NOT",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>subtask_a</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3299</th>\n      <td>Det er kraftedme en stor præstation der her, e...</td>\n      <td>OFF</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>De små får fri på vores skole fordi en knægt b...</td>\n      <td>OFF</td>\n    </tr>\n    <tr>\n      <th>2041</th>\n      <td>HEY! Bilar er jo sådan det eneste gode der er ...</td>\n      <td>OFF</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>@USER hvis hun ikke kan koge pastaen rigtigt, ...</td>\n      <td>OFF</td>\n    </tr>\n    <tr>\n      <th>962</th>\n      <td>Passiv aggressiv måde at kalde dig for et pikfjæs</td>\n      <td>OFF</td>\n    </tr>\n    <tr>\n      <th>799</th>\n      <td>hvorfor i den fucking store helvede skal man f...</td>\n      <td>OFF</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>Det sgu heller ikke okay. jeg havde sgu også b...</td>\n      <td>OFF</td>\n    </tr>\n    <tr>\n      <th>987</th>\n      <td>Tak, fordi du ikke vanærede @USER ved at sætte...</td>\n      <td>OFF</td>\n    </tr>\n    <tr>\n      <th>1251</th>\n      <td>Han EJER ikke respekt for nogen eller noget......</td>\n      <td>OFF</td>\n    </tr>\n    <tr>\n      <th>1167</th>\n      <td>Lækkert lorteindslag v1, jeg giver d1 1/1.</td>\n      <td>OFF</td>\n    </tr>\n    <tr>\n      <th>581</th>\n      <td>Biograferne tjener absolut intet på billetten ...</td>\n      <td>NOT</td>\n    </tr>\n    <tr>\n      <th>2643</th>\n      <td>lol</td>\n      <td>NOT</td>\n    </tr>\n    <tr>\n      <th>1889</th>\n      <td>###DANSKEN ER EN DEJLIG MAND!!!</td>\n      <td>NOT</td>\n    </tr>\n    <tr>\n      <th>632</th>\n      <td>DET ER helt fint at de mænd der gerne vil have...</td>\n      <td>NOT</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>@USER siger det går ufatteligt godt i Danmark....</td>\n      <td>NOT</td>\n    </tr>\n    <tr>\n      <th>1431</th>\n      <td>@USER, du snakker udenom, og du gør det med vi...</td>\n      <td>NOT</td>\n    </tr>\n    <tr>\n      <th>2080</th>\n      <td>Hvis jeg elsker Danmark og hader gud er det så...</td>\n      <td>NOT</td>\n    </tr>\n    <tr>\n      <th>633</th>\n      <td>Og hvad med Danelagen? St. Croix, St. Thomas o...</td>\n      <td>NOT</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1 million sort om året, sætter sit præg på til...</td>\n      <td>NOT</td>\n    </tr>\n    <tr>\n      <th>1038</th>\n      <td>Endnu en af Stampes berigere… Få dem dog ud!....</td>\n      <td>NOT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "mini_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "For this dataset we will be using a model trained on the train set of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"DaNLP/da-bert-hatespeech-detection\"\n",
    "pipe = pipeline(\"sentiment-analysis\", # text classification == sentiment analysis (don't ask me why, but they removed textcat in the latest version)\n",
    "               model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly check the output using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'label': 'offensive', 'score': 0.9902199506759644},\n {'label': 'not offensive', 'score': 0.9998297691345215}]"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "pipe([\"Gamle stupide idiot\", \"Lækkert vejr i dag\"]) # old stupid idiot, nice weather today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly apply this model to all our examples and save them in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = mini_test[\"tweet\"].to_list()\n",
    "\n",
    "def apply(texts):\n",
    "    output = pipe(texts, truncation=True)\n",
    "    return [t[\"score\"] if t[\"label\"] == \"offensive\" else 1 - t[\"score\"] for t in output]\n",
    "\n",
    "\n",
    "# first without augmentations\n",
    "mini_test[\"p_offensive_no_aug\"] = apply(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioural check using Augmentation\n",
    "\n",
    "In the following we want to examine the behavioural consistency of the model using augmentation. The idea is to check the behavioural consistently of the model for instance if we introduce slight spelling errors we the model should still be able to recognize names. If this is not the case it might be unwise to apply the model to domains where spelling errors are common such as social media.  \n",
    "\n",
    "![](img/aug.png)\n",
    "**Figure 1**: Examples of augmentation applied by Enevoldsen et al. (2020) and what domains they might be of relevance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenty\n",
    "For the augmentation we will be using the package augmenty, the following provides a brief introduction to it.\n",
    "\n",
    "**NOTE**: You are naturally not forced to use augmenty, you implement your own augmenters i.e. the following example with uppercasing is easy to implement by hand.  For example if you want to examine the effect of questionmarks you could make the augmentation:\n",
    "```py\n",
    "q_aug = [text + \"?\" for text in texts]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "spacy.orth_variants.v1\nspacy.lower_case.v1\nrandom_casing.v1\nchar_replace_random.v1\nchar_replace.v1\nkeystroke_error.v1\nremove_spacing.v1\nchar_swap.v1\nrandom_starting_case.v1\nconditional_token_casing.v1\ntoken_dict_replace.v1\nwordnet_synonym.v1\ntoken_replace.v1\nword_embedding.v1\ngrundtvigian_spacing_augmenter.v1\nspacing_insertion.v1\ntoken_swap.v1\ntoken_insert.v1\ntoken_insert_random.v1\nduplicate_token.v1\nrandom_synonym_insertion.v1\nents_replace.v1\nper_replace.v1\nents_format.v1\nupper_case.v1\nspongebob.v1\nda_æøå_replace.v1\nda_historical_noun_casing.v1\n"
    }
   ],
   "source": [
    "import augmenty\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"da_core_news_lg\")\n",
    "\n",
    "# a list of augmenters\n",
    "for augmenter in augmenty.augmenters():\n",
    "    print(augmenter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list naturally does not give you all the information you need. You can always examine a specific augmenter more en detain in the [documentation](https://kennethenevoldsen.github.io/augmenty/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try one of the augmenters. We can use the `augmenty.load` as a common interface for all augmenters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an augmenter\n",
    "upper_case_augmenter = augmenty.load(\"upper_case.v1\", level=1.00) # augment 100% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_synonym = augmenty.load(\"random_synonym_insertion.v1\", level=1.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svampebobben = augmenty.load(\"spongebob.v1\", level=1.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These augmenters are made to work on the SpaCy data class Examples which allows for much more detailed augmentation, however augmenty have utility function to allow us to use them for strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ThIs iS An eXaMpLe', 'AnD AnOtHeR OnE']"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "examples = [\"this is an example\", \"and another one\"]\n",
    "aug_texts = augmenty.texts(examples, augmenter=svampebobben, nlp=nlp)\n",
    "list(aug_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is uppercasing more offensive?\n",
    "\n",
    "Now we will can apply our model to the augmented examples to see if it changes predictions of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_texts = augmenty.texts(texts, augmenter=random_synonym, nlp=nlp)\n",
    "mini_test[\"p_offensive_upper\"] = apply(list(aug_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the output of our models we quickly see that it doesn't change the result at all! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                  tweet subtask_a  \\\nid                                                                  \n3299  Det er kraftedme en stor præstation der her, e...       OFF   \n305   De små får fri på vores skole fordi en knægt b...       OFF   \n2041  HEY! Bilar er jo sådan det eneste gode der er ...       OFF   \n701   @USER hvis hun ikke kan koge pastaen rigtigt, ...       OFF   \n962   Passiv aggressiv måde at kalde dig for et pikfjæs       OFF   \n799   hvorfor i den fucking store helvede skal man f...       OFF   \n519   Det sgu heller ikke okay. jeg havde sgu også b...       OFF   \n987   Tak, fordi du ikke vanærede @USER ved at sætte...       OFF   \n1251  Han EJER ikke respekt for nogen eller noget......       OFF   \n1167         Lækkert lorteindslag v1, jeg giver d1 1/1.       OFF   \n581   Biograferne tjener absolut intet på billetten ...       NOT   \n2643                                                lol       NOT   \n1889                    ###DANSKEN ER EN DEJLIG MAND!!!       NOT   \n632   DET ER helt fint at de mænd der gerne vil have...       NOT   \n380   @USER siger det går ufatteligt godt i Danmark....       NOT   \n1431  @USER, du snakker udenom, og du gør det med vi...       NOT   \n2080  Hvis jeg elsker Danmark og hader gud er det så...       NOT   \n633   Og hvad med Danelagen? St. Croix, St. Thomas o...       NOT   \n23    1 million sort om året, sætter sit præg på til...       NOT   \n1038   Endnu en af Stampes berigere… Få dem dog ud!....       NOT   \n\n      p_offensive_no_aug  p_offensive_upper  \nid                                           \n3299            0.000571           0.000896  \n305             0.030936           0.014478  \n2041            0.014710           0.003938  \n701             0.973846           0.260128  \n962             0.963706           0.030268  \n799             0.595546           0.988877  \n519             0.000431           0.000431  \n987             0.965915           0.015198  \n1251            0.978986           0.972280  \n1167            0.004151           0.004151  \n581             0.001128           0.003528  \n2643            0.000281           0.000281  \n1889            0.000251           0.000251  \n632             0.976177           0.052482  \n380             0.035504           0.035504  \n1431            0.991821           0.022789  \n2080            0.001390           0.001454  \n633             0.000229           0.000229  \n23              0.000109           0.000443  \n1038            0.970846           0.006874  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>subtask_a</th>\n      <th>p_offensive_no_aug</th>\n      <th>p_offensive_upper</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3299</th>\n      <td>Det er kraftedme en stor præstation der her, e...</td>\n      <td>OFF</td>\n      <td>0.000571</td>\n      <td>0.000896</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>De små får fri på vores skole fordi en knægt b...</td>\n      <td>OFF</td>\n      <td>0.030936</td>\n      <td>0.014478</td>\n    </tr>\n    <tr>\n      <th>2041</th>\n      <td>HEY! Bilar er jo sådan det eneste gode der er ...</td>\n      <td>OFF</td>\n      <td>0.014710</td>\n      <td>0.003938</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>@USER hvis hun ikke kan koge pastaen rigtigt, ...</td>\n      <td>OFF</td>\n      <td>0.973846</td>\n      <td>0.260128</td>\n    </tr>\n    <tr>\n      <th>962</th>\n      <td>Passiv aggressiv måde at kalde dig for et pikfjæs</td>\n      <td>OFF</td>\n      <td>0.963706</td>\n      <td>0.030268</td>\n    </tr>\n    <tr>\n      <th>799</th>\n      <td>hvorfor i den fucking store helvede skal man f...</td>\n      <td>OFF</td>\n      <td>0.595546</td>\n      <td>0.988877</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>Det sgu heller ikke okay. jeg havde sgu også b...</td>\n      <td>OFF</td>\n      <td>0.000431</td>\n      <td>0.000431</td>\n    </tr>\n    <tr>\n      <th>987</th>\n      <td>Tak, fordi du ikke vanærede @USER ved at sætte...</td>\n      <td>OFF</td>\n      <td>0.965915</td>\n      <td>0.015198</td>\n    </tr>\n    <tr>\n      <th>1251</th>\n      <td>Han EJER ikke respekt for nogen eller noget......</td>\n      <td>OFF</td>\n      <td>0.978986</td>\n      <td>0.972280</td>\n    </tr>\n    <tr>\n      <th>1167</th>\n      <td>Lækkert lorteindslag v1, jeg giver d1 1/1.</td>\n      <td>OFF</td>\n      <td>0.004151</td>\n      <td>0.004151</td>\n    </tr>\n    <tr>\n      <th>581</th>\n      <td>Biograferne tjener absolut intet på billetten ...</td>\n      <td>NOT</td>\n      <td>0.001128</td>\n      <td>0.003528</td>\n    </tr>\n    <tr>\n      <th>2643</th>\n      <td>lol</td>\n      <td>NOT</td>\n      <td>0.000281</td>\n      <td>0.000281</td>\n    </tr>\n    <tr>\n      <th>1889</th>\n      <td>###DANSKEN ER EN DEJLIG MAND!!!</td>\n      <td>NOT</td>\n      <td>0.000251</td>\n      <td>0.000251</td>\n    </tr>\n    <tr>\n      <th>632</th>\n      <td>DET ER helt fint at de mænd der gerne vil have...</td>\n      <td>NOT</td>\n      <td>0.976177</td>\n      <td>0.052482</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>@USER siger det går ufatteligt godt i Danmark....</td>\n      <td>NOT</td>\n      <td>0.035504</td>\n      <td>0.035504</td>\n    </tr>\n    <tr>\n      <th>1431</th>\n      <td>@USER, du snakker udenom, og du gør det med vi...</td>\n      <td>NOT</td>\n      <td>0.991821</td>\n      <td>0.022789</td>\n    </tr>\n    <tr>\n      <th>2080</th>\n      <td>Hvis jeg elsker Danmark og hader gud er det så...</td>\n      <td>NOT</td>\n      <td>0.001390</td>\n      <td>0.001454</td>\n    </tr>\n    <tr>\n      <th>633</th>\n      <td>Og hvad med Danelagen? St. Croix, St. Thomas o...</td>\n      <td>NOT</td>\n      <td>0.000229</td>\n      <td>0.000229</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1 million sort om året, sætter sit præg på til...</td>\n      <td>NOT</td>\n      <td>0.000109</td>\n      <td>0.000443</td>\n    </tr>\n    <tr>\n      <th>1038</th>\n      <td>Endnu en af Stampes berigere… Få dem dog ud!....</td>\n      <td>NOT</td>\n      <td>0.970846</td>\n      <td>0.006874</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "mini_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be a bit more explicit we can also compare it using summary information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The augmentation lead to classification changes in 6/20\nThe average prob. of NOT went from 0.298(0.471) to 0.012(0.018).\nThe average prob. of OFF went from 0.453(0.48) to 0.229(0.404).\n"
    }
   ],
   "source": [
    "def compare_cols(\n",
    "    augmentation,\n",
    "    baseline=mini_test[\"p_offensive_no_aug\"],\n",
    "    category=mini_test[\"subtask_a\"],\n",
    "):\n",
    "    \"\"\"Compares augmentation with the baseline for each of the categories\"\"\"\n",
    "    changes = ((augmentation > 0.5) != (baseline > 0.5)).sum()\n",
    "    n = len(augmentation)\n",
    "    print(f\"The augmentation lead to classification changes in {changes}/{n}\")\n",
    "    for cat in set(category):\n",
    "        aug_cat_mean = augmentation[category == cat].mean().round(3)\n",
    "        aug_cat_std = augmentation[category == cat].std().round(3)\n",
    "        cat_mean = baseline[category == cat].mean().round(3)\n",
    "        cat_std = baseline[category == cat].std().round(3)\n",
    "        print(\n",
    "            f\"The average prob. of {cat} went from {cat_mean}({cat_std}) to {aug_cat_mean}({aug_cat_std}).\"\n",
    "        )\n",
    "\n",
    "compare_cols(mini_test[\"p_offensive_upper\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises:\n",
    "\n",
    "1) Solve the above mystery, why doesn't the model estimate change might when uppercasing? *Hint*: Check the tokenizer of the model\n",
    "2) Examining the data, I seemed to notice that spelling error were more common among offensive tweets. Is this correct? [*Hint*](https://kennethenevoldsen.github.io/augmenty/augmenty.character.html?highlight=keystroke#augmenty.character.replace.create_keystroke_error_augmenter)\n",
    "3) Examine the data yourself and create three hypothesis on what augmentation might change the performance.\n",
    "4) Outline how you could apply augmentation (behavioral testing) to examine a model (or pipeline) in your project\n",
    "5) (Optional): Apply this behavioural testing to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "021482b7625aaacc2d343324781c6ce2f121934a239bde69eda2b56fdffea080"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit4207b8f6ec6449f58b63b080cdef809d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}