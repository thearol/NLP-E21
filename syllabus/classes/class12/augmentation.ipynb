{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation using Augmentation\n",
    "For this class we will conduct model validation using augmentation, we will especially use the package [Augmenty](https://kennethenevoldsen.github.io/augmenty/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We will need to set up a few things before we start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages:\n",
    "For this tutorial you will need the following packages:\n",
    "\n",
    "- spaCy and augmenty are used for the augmentation\n",
    "- transformers are use to run the model we wish to validate\n",
    "- danlp is used to download the dataset we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install augmenty spacy==3.1.1 transformers==4.2.2 danlp==0.0.12\n",
    "# !spacy -m download da_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "For this dataset we will be using [DKHate](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dkhate). The DKHate dataset contains user-generated comments from social media platforms (Facebook and Reddit) annotated for various types and target of offensive language. Note that only labels for the sub-task A (Offensive language identification), i.e. NOT (Not Offensive) / OFF (Offensive), are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import DKHate\n",
    "import pandas as pd\n",
    "dkhate = DKHate()\n",
    "test, train = dkhate.load_with_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make everything run faster we will only be using a subsample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 20\n",
    "\n",
    "# make sure to sample evenly from the two samples\n",
    "n_labels = len(test[\"subtask_a\"].unique())\n",
    "samples_pr_lab = samples//n_labels\n",
    "\n",
    "off = test[test[\"subtask_a\"] == \"OFF\"].sample(samples_pr_lab)\n",
    "not_off = test[test[\"subtask_a\"] == \"NOT\"].sample(samples_pr_lab)\n",
    "mini_test = pd.concat([off, not_off])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the data using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>@USER ryger du hash. ???</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hvordan i helvede fik de overhovedet dit numme...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>[FUCK YOU!! INGLIN WILL RISE AGAIN!!](URL</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>Hold hold hold. Det er Tobias og hans kumpaner...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>De forældre har ikke lært opdragelse. Så de ka...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>Lækkert lorteindslag v1, jeg giver d1 1/1.</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>Tosse. Skulle have fløjet med helikopter. Han ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Normalt synes jeg Marx var lige højreorientere...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>Jeg ville ønske jeg kunne anerkende [KORRUPT P...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>hvorfor i den fucking store helvede skal man f...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>Endnu en af Stampes berigere… Få dem dog ud!....</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Lever af sit udseende, vil ikke vise sig frem</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>Gad vide om hans næste arbejde bliver McKinsey...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>Gu skal vi ej hjælpe dem mere, de for rigeligt...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>Den er god nok, det er lige udenfor Spjald, de...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>Jeg elsker dig!</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>Det er da synd at de skal associeres med sådan...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396</th>\n",
       "      <td>Hahaha. Stærkt.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>Ja bare hjem med dem</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Ja tak</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet subtask_a\n",
       "id                                                               \n",
       "137                            @USER ryger du hash. ???       OFF\n",
       "9     Hvordan i helvede fik de overhovedet dit numme...       OFF\n",
       "1425          [FUCK YOU!! INGLIN WILL RISE AGAIN!!](URL       OFF\n",
       "534   Hold hold hold. Det er Tobias og hans kumpaner...       OFF\n",
       "1285  De forældre har ikke lært opdragelse. Så de ka...       OFF\n",
       "1167         Lækkert lorteindslag v1, jeg giver d1 1/1.       OFF\n",
       "647   Tosse. Skulle have fløjet med helikopter. Han ...       OFF\n",
       "544   Normalt synes jeg Marx var lige højreorientere...       OFF\n",
       "924   Jeg ville ønske jeg kunne anerkende [KORRUPT P...       OFF\n",
       "799   hvorfor i den fucking store helvede skal man f...       OFF\n",
       "1038   Endnu en af Stampes berigere… Få dem dog ud!....       NOT\n",
       "73        Lever af sit udseende, vil ikke vise sig frem       NOT\n",
       "2895  Gad vide om hans næste arbejde bliver McKinsey...       NOT\n",
       "459   Gu skal vi ej hjælpe dem mere, de for rigeligt...       NOT\n",
       "1220  Den er god nok, det er lige udenfor Spjald, de...       NOT\n",
       "2560                                    Jeg elsker dig!       NOT\n",
       "3522  Det er da synd at de skal associeres med sådan...       NOT\n",
       "3396                                    Hahaha. Stærkt.       NOT\n",
       "883                                Ja bare hjem med dem       NOT\n",
       "173                                              Ja tak       NOT"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "For this dataset we will be using a model trained on the train set of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"DaNLP/da-bert-hatespeech-detection\"\n",
    "pipe = pipeline(\"sentiment-analysis\", # text classification == sentiment analysis (don't ask me why, but they removed textcat in the latest version)\n",
    "               model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly check the output using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'offensive', 'score': 0.9902198910713196},\n",
       " {'label': 'not offensive', 'score': 0.9998297691345215}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe([\"Gamle stupide idiot\", \"Lækkert vejr i dag\"]) # old stupid idiot, nice weather today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly apply this model to all our examples and save them in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = mini_test[\"tweet\"].to_list()\n",
    "\n",
    "def apply(texts):\n",
    "    output = pipe(texts, truncation=True)\n",
    "    return [t[\"score\"] if t[\"label\"] == \"offensive\" else 1 - t[\"score\"] for t in output]\n",
    "\n",
    "\n",
    "# first without augmentations\n",
    "mini_test[\"p_offensive_no_aug\"] = apply(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioural check using Augmentation\n",
    "\n",
    "In the following we want to examine the behavioural consistency of the model using augmentation. The idea is to check the behavioural consistently of the model for instance if we introduce slight spelling errors we the model should still be able to recognize names. If this is not the case it might be unwise to apply the model to domains where spelling errors are common such as social media.  \n",
    "\n",
    "![](img/aug.png)\n",
    "**Figure 1**: Examples of augmentation applied by Enevoldsen et al. (2020) and what domains they might be of relevance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenty\n",
    "For the augmentation we will be using the package augmenty, the following provides a brief introduction to it.\n",
    "\n",
    "**NOTE**: You are naturally not forced to use augmenty, you implement your own augmenters i.e. the following example with uppercasing is easy to implement by hand.  For example if you want to examine the effect of questionmarks you could make the augmentation:\n",
    "```py\n",
    "q_aug = [text + \"?\" for text in texts]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/au561649/.virtualenvs/NLP/lib/python3.8/site-packages/spacy/util.py:730: UserWarning: [W095] Model 'da_core_news_lg' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy.orth_variants.v1\n",
      "spacy.lower_case.v1\n",
      "random_casing.v1\n",
      "char_replace_random.v1\n",
      "char_replace.v1\n",
      "keystroke_error.v1\n",
      "remove_spacing.v1\n",
      "char_swap.v1\n",
      "random_starting_case.v1\n",
      "conditional_token_casing.v1\n",
      "token_dict_replace.v1\n",
      "wordnet_synonym.v1\n",
      "token_replace.v1\n",
      "word_embedding.v1\n",
      "grundtvigian_spacing_augmenter.v1\n",
      "spacing_insertion.v1\n",
      "token_swap.v1\n",
      "token_insert.v1\n",
      "token_insert_random.v1\n",
      "duplicate_token.v1\n",
      "random_synonym_insertion.v1\n",
      "ents_replace.v1\n",
      "per_replace.v1\n",
      "ents_format.v1\n",
      "upper_case.v1\n",
      "spongebob.v1\n",
      "da_æøå_replace.v1\n",
      "da_historical_noun_casing.v1\n"
     ]
    }
   ],
   "source": [
    "import augmenty\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"da_core_news_lg\")\n",
    "\n",
    "# a list of augmenters\n",
    "for augmenter in augmenty.augmenters():\n",
    "    print(augmenter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list naturally does not give you all the information you need. You can always examine a specific augmenter more en detain in the [documentation](https://kennethenevoldsen.github.io/augmenty/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try one of the augmenters. We can use the `augmenty.load` as a common interface for all augmenters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an augmenter\n",
    "upper_case_augmenter = augmenty.load(\"upper_case.v1\", level=1.00) # augment 100% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These augmenters are made to work on the SpaCy data class Examples which allows for much more detailed augmentation, however augmenty have utility function to allow us to use them for strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THIS IS AN EXAMPLE', 'AND ANOTHER ONE']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\"this is an example\", \"and another one\"]\n",
    "aug_texts = augmenty.texts(examples, augmenter=upper_case_augmenter, nlp=nlp)\n",
    "list(aug_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is uppercasing more offensive?\n",
    "\n",
    "Now we will can apply our model to the augmented examples to see if it changes predictions of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_texts = augmenty.texts(texts, augmenter=upper_case_augmenter, nlp=nlp)\n",
    "mini_test[\"p_offensive_upper\"] = apply(list(aug_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the output of our models we quickly see that it doesn't change the result at all! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>p_offensive_no_aug</th>\n",
       "      <th>p_offensive_upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>@USER ryger du hash. ???</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.997740</td>\n",
       "      <td>0.997740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hvordan i helvede fik de overhovedet dit numme...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.972994</td>\n",
       "      <td>0.972994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>[FUCK YOU!! INGLIN WILL RISE AGAIN!!](URL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.983403</td>\n",
       "      <td>0.983403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>Hold hold hold. Det er Tobias og hans kumpaner...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.840630</td>\n",
       "      <td>0.840630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>De forældre har ikke lært opdragelse. Så de ka...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>0.004568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>Lækkert lorteindslag v1, jeg giver d1 1/1.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>0.004151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>Tosse. Skulle have fløjet med helikopter. Han ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.956662</td>\n",
       "      <td>0.956662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Normalt synes jeg Marx var lige højreorientere...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.125608</td>\n",
       "      <td>0.125608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>Jeg ville ønske jeg kunne anerkende [KORRUPT P...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.993760</td>\n",
       "      <td>0.993760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>hvorfor i den fucking store helvede skal man f...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.595557</td>\n",
       "      <td>0.595557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>Endnu en af Stampes berigere… Få dem dog ud!....</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.970846</td>\n",
       "      <td>0.970846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Lever af sit udseende, vil ikke vise sig frem</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>Gad vide om hans næste arbejde bliver McKinsey...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>Gu skal vi ej hjælpe dem mere, de for rigeligt...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>Den er god nok, det er lige udenfor Spjald, de...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>Jeg elsker dig!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>Det er da synd at de skal associeres med sådan...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.003357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396</th>\n",
       "      <td>Hahaha. Stærkt.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>Ja bare hjem med dem</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.002111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Ja tak</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet subtask_a  \\\n",
       "id                                                                  \n",
       "137                            @USER ryger du hash. ???       OFF   \n",
       "9     Hvordan i helvede fik de overhovedet dit numme...       OFF   \n",
       "1425          [FUCK YOU!! INGLIN WILL RISE AGAIN!!](URL       OFF   \n",
       "534   Hold hold hold. Det er Tobias og hans kumpaner...       OFF   \n",
       "1285  De forældre har ikke lært opdragelse. Så de ka...       OFF   \n",
       "1167         Lækkert lorteindslag v1, jeg giver d1 1/1.       OFF   \n",
       "647   Tosse. Skulle have fløjet med helikopter. Han ...       OFF   \n",
       "544   Normalt synes jeg Marx var lige højreorientere...       OFF   \n",
       "924   Jeg ville ønske jeg kunne anerkende [KORRUPT P...       OFF   \n",
       "799   hvorfor i den fucking store helvede skal man f...       OFF   \n",
       "1038   Endnu en af Stampes berigere… Få dem dog ud!....       NOT   \n",
       "73        Lever af sit udseende, vil ikke vise sig frem       NOT   \n",
       "2895  Gad vide om hans næste arbejde bliver McKinsey...       NOT   \n",
       "459   Gu skal vi ej hjælpe dem mere, de for rigeligt...       NOT   \n",
       "1220  Den er god nok, det er lige udenfor Spjald, de...       NOT   \n",
       "2560                                    Jeg elsker dig!       NOT   \n",
       "3522  Det er da synd at de skal associeres med sådan...       NOT   \n",
       "3396                                    Hahaha. Stærkt.       NOT   \n",
       "883                                Ja bare hjem med dem       NOT   \n",
       "173                                              Ja tak       NOT   \n",
       "\n",
       "      p_offensive_no_aug  p_offensive_upper  \n",
       "id                                           \n",
       "137             0.997740           0.997740  \n",
       "9               0.972994           0.972994  \n",
       "1425            0.983403           0.983403  \n",
       "534             0.840630           0.840630  \n",
       "1285            0.004568           0.004568  \n",
       "1167            0.004151           0.004151  \n",
       "647             0.956662           0.956662  \n",
       "544             0.125608           0.125608  \n",
       "924             0.993760           0.993760  \n",
       "799             0.595557           0.595557  \n",
       "1038            0.970846           0.970846  \n",
       "73              0.000437           0.000437  \n",
       "2895            0.000316           0.000316  \n",
       "459             0.000070           0.000070  \n",
       "1220            0.000336           0.000336  \n",
       "2560            0.000485           0.000485  \n",
       "3522            0.003357           0.003357  \n",
       "3396            0.000142           0.000142  \n",
       "883             0.002111           0.002111  \n",
       "173             0.000051           0.000051  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be a bit more explicit we can also compare it using summary information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The augmentation lead to classification changes in 0/20\n",
      "The average prob. of NOT went from 0.098(0.307) to 0.098(0.307).\n",
      "The average prob. of OFF went from 0.648(0.434) to 0.648(0.434).\n"
     ]
    }
   ],
   "source": [
    "def compare_cols(\n",
    "    augmentation,\n",
    "    baseline=mini_test[\"p_offensive_no_aug\"],\n",
    "    category=mini_test[\"subtask_a\"],\n",
    "):\n",
    "    \"\"\"Compares augmentation with the baseline for each of the categories\"\"\"\n",
    "    changes = ((augmentation > 0.5) != (baseline > 0.5)).sum()\n",
    "    n = len(augmentation)\n",
    "    print(f\"The augmentation lead to classification changes in {changes}/{n}\")\n",
    "    for cat in set(category):\n",
    "        aug_cat_mean = augmentation[category == cat].mean().round(3)\n",
    "        aug_cat_std = augmentation[category == cat].std().round(3)\n",
    "        cat_mean = baseline[category == cat].mean().round(3)\n",
    "        cat_std = baseline[category == cat].std().round(3)\n",
    "        print(\n",
    "            f\"The average prob. of {cat} went from {cat_mean}({cat_std}) to {aug_cat_mean}({aug_cat_std}).\"\n",
    "        )\n",
    "\n",
    "compare_cols(mini_test[\"p_offensive_upper\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises:\n",
    "\n",
    "1) Solve the above mystery, why doesn't the model estimate change might when uppercasing? *Hint*: Check the tokenizer of the model\n",
    "2) Examining the data, I seemed to notice that spelling error were more common among offensive tweets. Is this correct? [*Hint*](https://kennethenevoldsen.github.io/augmenty/augmenty.character.html?highlight=keystroke#augmenty.character.replace.create_keystroke_error_augmenter)\n",
    "3) Examine the data yourself and create three hypothesis on what augmentation might change the performance.\n",
    "4) Outline how you could apply augmentation (behavioral testing) to examine a model (or pipeline) in your project\n",
    "5) (Optional): Apply this behavioural testing to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "021482b7625aaacc2d343324781c6ce2f121934a239bde69eda2b56fdffea080"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('NLP': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
